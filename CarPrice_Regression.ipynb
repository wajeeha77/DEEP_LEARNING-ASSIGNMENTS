{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "civil-paint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     car_ID  symboling                       CarName fueltype aspiration  \\\n",
      "0         1          3            alfa-romero giulia      gas        std   \n",
      "1         2          3           alfa-romero stelvio      gas        std   \n",
      "2         3          1      alfa-romero Quadrifoglio      gas        std   \n",
      "3         4          2                   audi 100 ls      gas        std   \n",
      "4         5          2                    audi 100ls      gas        std   \n",
      "5         6          2                      audi fox      gas        std   \n",
      "6         7          1                    audi 100ls      gas        std   \n",
      "7         8          1                     audi 5000      gas        std   \n",
      "8         9          1                     audi 4000      gas      turbo   \n",
      "9        10          0           audi 5000s (diesel)      gas      turbo   \n",
      "10       11          2                      bmw 320i      gas        std   \n",
      "11       12          0                      bmw 320i      gas        std   \n",
      "12       13          0                        bmw x1      gas        std   \n",
      "13       14          0                        bmw x3      gas        std   \n",
      "14       15          1                        bmw z4      gas        std   \n",
      "15       16          0                        bmw x4      gas        std   \n",
      "16       17          0                        bmw x5      gas        std   \n",
      "17       18          0                        bmw x3      gas        std   \n",
      "18       19          2              chevrolet impala      gas        std   \n",
      "19       20          1         chevrolet monte carlo      gas        std   \n",
      "20       21          0           chevrolet vega 2300      gas        std   \n",
      "21       22          1                 dodge rampage      gas        std   \n",
      "22       23          1           dodge challenger se      gas        std   \n",
      "23       24          1                    dodge d200      gas      turbo   \n",
      "24       25          1             dodge monaco (sw)      gas        std   \n",
      "25       26          1            dodge colt hardtop      gas        std   \n",
      "26       27          1               dodge colt (sw)      gas        std   \n",
      "27       28          1          dodge coronet custom      gas      turbo   \n",
      "28       29         -1             dodge dart custom      gas        std   \n",
      "29       30          3     dodge coronet custom (sw)      gas      turbo   \n",
      "..      ...        ...                           ...      ...        ...   \n",
      "175     176         -1                 toyota corona      gas        std   \n",
      "176     177         -1                toyota corolla      gas        std   \n",
      "177     178         -1                toyota mark ii      gas        std   \n",
      "178     179          3       toyota corolla liftback      gas        std   \n",
      "179     180          3                 toyota corona      gas        std   \n",
      "180     181         -1                toyota starlet      gas        std   \n",
      "181     182         -1                toyouta tercel      gas        std   \n",
      "182     183          2              vokswagen rabbit   diesel        std   \n",
      "183     184          2  volkswagen 1131 deluxe sedan      gas        std   \n",
      "184     185          2          volkswagen model 111   diesel        std   \n",
      "185     186          2             volkswagen type 3      gas        std   \n",
      "186     187          2           volkswagen 411 (sw)      gas        std   \n",
      "187     188          2       volkswagen super beetle   diesel      turbo   \n",
      "188     189          2             volkswagen dasher      gas        std   \n",
      "189     190          3                     vw dasher      gas        std   \n",
      "190     191          3                     vw rabbit      gas        std   \n",
      "191     192          0             volkswagen rabbit      gas        std   \n",
      "192     193          0      volkswagen rabbit custom   diesel      turbo   \n",
      "193     194          0             volkswagen dasher      gas        std   \n",
      "194     195         -2               volvo 145e (sw)      gas        std   \n",
      "195     196         -1                   volvo 144ea      gas        std   \n",
      "196     197         -2                   volvo 244dl      gas        std   \n",
      "197     198         -1                     volvo 245      gas        std   \n",
      "198     199         -2                   volvo 264gl      gas      turbo   \n",
      "199     200         -1                  volvo diesel      gas      turbo   \n",
      "200     201         -1               volvo 145e (sw)      gas        std   \n",
      "201     202         -1                   volvo 144ea      gas      turbo   \n",
      "202     203         -1                   volvo 244dl      gas        std   \n",
      "203     204         -1                     volvo 246   diesel      turbo   \n",
      "204     205         -1                   volvo 264gl      gas      turbo   \n",
      "\n",
      "    doornumber      carbody drivewheel enginelocation  wheelbase  ...  \\\n",
      "0          two  convertible        rwd          front       88.6  ...   \n",
      "1          two  convertible        rwd          front       88.6  ...   \n",
      "2          two    hatchback        rwd          front       94.5  ...   \n",
      "3         four        sedan        fwd          front       99.8  ...   \n",
      "4         four        sedan        4wd          front       99.4  ...   \n",
      "5          two        sedan        fwd          front       99.8  ...   \n",
      "6         four        sedan        fwd          front      105.8  ...   \n",
      "7         four        wagon        fwd          front      105.8  ...   \n",
      "8         four        sedan        fwd          front      105.8  ...   \n",
      "9          two    hatchback        4wd          front       99.5  ...   \n",
      "10         two        sedan        rwd          front      101.2  ...   \n",
      "11        four        sedan        rwd          front      101.2  ...   \n",
      "12         two        sedan        rwd          front      101.2  ...   \n",
      "13        four        sedan        rwd          front      101.2  ...   \n",
      "14        four        sedan        rwd          front      103.5  ...   \n",
      "15        four        sedan        rwd          front      103.5  ...   \n",
      "16         two        sedan        rwd          front      103.5  ...   \n",
      "17        four        sedan        rwd          front      110.0  ...   \n",
      "18         two    hatchback        fwd          front       88.4  ...   \n",
      "19         two    hatchback        fwd          front       94.5  ...   \n",
      "20        four        sedan        fwd          front       94.5  ...   \n",
      "21         two    hatchback        fwd          front       93.7  ...   \n",
      "22         two    hatchback        fwd          front       93.7  ...   \n",
      "23         two    hatchback        fwd          front       93.7  ...   \n",
      "24        four    hatchback        fwd          front       93.7  ...   \n",
      "25        four        sedan        fwd          front       93.7  ...   \n",
      "26        four        sedan        fwd          front       93.7  ...   \n",
      "27         two        sedan        fwd          front       93.7  ...   \n",
      "28        four        wagon        fwd          front      103.3  ...   \n",
      "29         two    hatchback        fwd          front       95.9  ...   \n",
      "..         ...          ...        ...            ...        ...  ...   \n",
      "175       four    hatchback        fwd          front      102.4  ...   \n",
      "176       four        sedan        fwd          front      102.4  ...   \n",
      "177       four    hatchback        fwd          front      102.4  ...   \n",
      "178        two    hatchback        rwd          front      102.9  ...   \n",
      "179        two    hatchback        rwd          front      102.9  ...   \n",
      "180       four        sedan        rwd          front      104.5  ...   \n",
      "181       four        wagon        rwd          front      104.5  ...   \n",
      "182        two        sedan        fwd          front       97.3  ...   \n",
      "183        two        sedan        fwd          front       97.3  ...   \n",
      "184       four        sedan        fwd          front       97.3  ...   \n",
      "185       four        sedan        fwd          front       97.3  ...   \n",
      "186       four        sedan        fwd          front       97.3  ...   \n",
      "187       four        sedan        fwd          front       97.3  ...   \n",
      "188       four        sedan        fwd          front       97.3  ...   \n",
      "189        two  convertible        fwd          front       94.5  ...   \n",
      "190        two    hatchback        fwd          front       94.5  ...   \n",
      "191       four        sedan        fwd          front      100.4  ...   \n",
      "192       four        sedan        fwd          front      100.4  ...   \n",
      "193       four        wagon        fwd          front      100.4  ...   \n",
      "194       four        sedan        rwd          front      104.3  ...   \n",
      "195       four        wagon        rwd          front      104.3  ...   \n",
      "196       four        sedan        rwd          front      104.3  ...   \n",
      "197       four        wagon        rwd          front      104.3  ...   \n",
      "198       four        sedan        rwd          front      104.3  ...   \n",
      "199       four        wagon        rwd          front      104.3  ...   \n",
      "200       four        sedan        rwd          front      109.1  ...   \n",
      "201       four        sedan        rwd          front      109.1  ...   \n",
      "202       four        sedan        rwd          front      109.1  ...   \n",
      "203       four        sedan        rwd          front      109.1  ...   \n",
      "204       four        sedan        rwd          front      109.1  ...   \n",
      "\n",
      "     enginesize  fuelsystem  boreratio  stroke compressionratio horsepower  \\\n",
      "0           130        mpfi       3.47    2.68             9.00        111   \n",
      "1           130        mpfi       3.47    2.68             9.00        111   \n",
      "2           152        mpfi       2.68    3.47             9.00        154   \n",
      "3           109        mpfi       3.19    3.40            10.00        102   \n",
      "4           136        mpfi       3.19    3.40             8.00        115   \n",
      "5           136        mpfi       3.19    3.40             8.50        110   \n",
      "6           136        mpfi       3.19    3.40             8.50        110   \n",
      "7           136        mpfi       3.19    3.40             8.50        110   \n",
      "8           131        mpfi       3.13    3.40             8.30        140   \n",
      "9           131        mpfi       3.13    3.40             7.00        160   \n",
      "10          108        mpfi       3.50    2.80             8.80        101   \n",
      "11          108        mpfi       3.50    2.80             8.80        101   \n",
      "12          164        mpfi       3.31    3.19             9.00        121   \n",
      "13          164        mpfi       3.31    3.19             9.00        121   \n",
      "14          164        mpfi       3.31    3.19             9.00        121   \n",
      "15          209        mpfi       3.62    3.39             8.00        182   \n",
      "16          209        mpfi       3.62    3.39             8.00        182   \n",
      "17          209        mpfi       3.62    3.39             8.00        182   \n",
      "18           61        2bbl       2.91    3.03             9.50         48   \n",
      "19           90        2bbl       3.03    3.11             9.60         70   \n",
      "20           90        2bbl       3.03    3.11             9.60         70   \n",
      "21           90        2bbl       2.97    3.23             9.41         68   \n",
      "22           90        2bbl       2.97    3.23             9.40         68   \n",
      "23           98        mpfi       3.03    3.39             7.60        102   \n",
      "24           90        2bbl       2.97    3.23             9.40         68   \n",
      "25           90        2bbl       2.97    3.23             9.40         68   \n",
      "26           90        2bbl       2.97    3.23             9.40         68   \n",
      "27           98        mpfi       3.03    3.39             7.60        102   \n",
      "28          122        2bbl       3.34    3.46             8.50         88   \n",
      "29          156         mfi       3.60    3.90             7.00        145   \n",
      "..          ...         ...        ...     ...              ...        ...   \n",
      "175         122        mpfi       3.31    3.54             8.70         92   \n",
      "176         122        mpfi       3.31    3.54             8.70         92   \n",
      "177         122        mpfi       3.31    3.54             8.70         92   \n",
      "178         171        mpfi       3.27    3.35             9.30        161   \n",
      "179         171        mpfi       3.27    3.35             9.30        161   \n",
      "180         171        mpfi       3.27    3.35             9.20        156   \n",
      "181         161        mpfi       3.27    3.35             9.20        156   \n",
      "182          97         idi       3.01    3.40            23.00         52   \n",
      "183         109        mpfi       3.19    3.40             9.00         85   \n",
      "184          97         idi       3.01    3.40            23.00         52   \n",
      "185         109        mpfi       3.19    3.40             9.00         85   \n",
      "186         109        mpfi       3.19    3.40             9.00         85   \n",
      "187          97         idi       3.01    3.40            23.00         68   \n",
      "188         109        mpfi       3.19    3.40            10.00        100   \n",
      "189         109        mpfi       3.19    3.40             8.50         90   \n",
      "190         109        mpfi       3.19    3.40             8.50         90   \n",
      "191         136        mpfi       3.19    3.40             8.50        110   \n",
      "192          97         idi       3.01    3.40            23.00         68   \n",
      "193         109        mpfi       3.19    3.40             9.00         88   \n",
      "194         141        mpfi       3.78    3.15             9.50        114   \n",
      "195         141        mpfi       3.78    3.15             9.50        114   \n",
      "196         141        mpfi       3.78    3.15             9.50        114   \n",
      "197         141        mpfi       3.78    3.15             9.50        114   \n",
      "198         130        mpfi       3.62    3.15             7.50        162   \n",
      "199         130        mpfi       3.62    3.15             7.50        162   \n",
      "200         141        mpfi       3.78    3.15             9.50        114   \n",
      "201         141        mpfi       3.78    3.15             8.70        160   \n",
      "202         173        mpfi       3.58    2.87             8.80        134   \n",
      "203         145         idi       3.01    3.40            23.00        106   \n",
      "204         141        mpfi       3.78    3.15             9.50        114   \n",
      "\n",
      "     peakrpm citympg  highwaympg      price  \n",
      "0       5000      21          27  13495.000  \n",
      "1       5000      21          27  16500.000  \n",
      "2       5000      19          26  16500.000  \n",
      "3       5500      24          30  13950.000  \n",
      "4       5500      18          22  17450.000  \n",
      "5       5500      19          25  15250.000  \n",
      "6       5500      19          25  17710.000  \n",
      "7       5500      19          25  18920.000  \n",
      "8       5500      17          20  23875.000  \n",
      "9       5500      16          22  17859.167  \n",
      "10      5800      23          29  16430.000  \n",
      "11      5800      23          29  16925.000  \n",
      "12      4250      21          28  20970.000  \n",
      "13      4250      21          28  21105.000  \n",
      "14      4250      20          25  24565.000  \n",
      "15      5400      16          22  30760.000  \n",
      "16      5400      16          22  41315.000  \n",
      "17      5400      15          20  36880.000  \n",
      "18      5100      47          53   5151.000  \n",
      "19      5400      38          43   6295.000  \n",
      "20      5400      38          43   6575.000  \n",
      "21      5500      37          41   5572.000  \n",
      "22      5500      31          38   6377.000  \n",
      "23      5500      24          30   7957.000  \n",
      "24      5500      31          38   6229.000  \n",
      "25      5500      31          38   6692.000  \n",
      "26      5500      31          38   7609.000  \n",
      "27      5500      24          30   8558.000  \n",
      "28      5000      24          30   8921.000  \n",
      "29      5000      19          24  12964.000  \n",
      "..       ...     ...         ...        ...  \n",
      "175     4200      27          32   9988.000  \n",
      "176     4200      27          32  10898.000  \n",
      "177     4200      27          32  11248.000  \n",
      "178     5200      20          24  16558.000  \n",
      "179     5200      19          24  15998.000  \n",
      "180     5200      20          24  15690.000  \n",
      "181     5200      19          24  15750.000  \n",
      "182     4800      37          46   7775.000  \n",
      "183     5250      27          34   7975.000  \n",
      "184     4800      37          46   7995.000  \n",
      "185     5250      27          34   8195.000  \n",
      "186     5250      27          34   8495.000  \n",
      "187     4500      37          42   9495.000  \n",
      "188     5500      26          32   9995.000  \n",
      "189     5500      24          29  11595.000  \n",
      "190     5500      24          29   9980.000  \n",
      "191     5500      19          24  13295.000  \n",
      "192     4500      33          38  13845.000  \n",
      "193     5500      25          31  12290.000  \n",
      "194     5400      23          28  12940.000  \n",
      "195     5400      23          28  13415.000  \n",
      "196     5400      24          28  15985.000  \n",
      "197     5400      24          28  16515.000  \n",
      "198     5100      17          22  18420.000  \n",
      "199     5100      17          22  18950.000  \n",
      "200     5400      23          28  16845.000  \n",
      "201     5300      19          25  19045.000  \n",
      "202     5500      18          23  21485.000  \n",
      "203     4800      26          27  22470.000  \n",
      "204     5400      19          25  22625.000  \n",
      "\n",
      "[205 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv (\"D:/CarPrice_Assignment.csv\")\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ordinary-clothing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "car_ID              0\n",
       "symboling           0\n",
       "CarName             0\n",
       "fueltype            0\n",
       "aspiration          0\n",
       "doornumber          0\n",
       "carbody             0\n",
       "drivewheel          0\n",
       "enginelocation      0\n",
       "wheelbase           0\n",
       "carlength           0\n",
       "carwidth            0\n",
       "carheight           0\n",
       "curbweight          0\n",
       "enginetype          0\n",
       "cylindernumber      0\n",
       "enginesize          0\n",
       "fuelsystem          0\n",
       "boreratio           0\n",
       "stroke              0\n",
       "compressionratio    0\n",
       "horsepower          0\n",
       "peakrpm             0\n",
       "citympg             0\n",
       "highwaympg          0\n",
       "price               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stylish-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "Enc = LabelEncoder()\n",
    "cat_df = df.select_dtypes(include='object')\n",
    "object_columns = cat_df.columns\n",
    "for attribute in object_columns:\n",
    "    Enc.fit(df[attribute])\n",
    "    df[attribute] = Enc.transform(df[attribute])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confirmed-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "corrmat = df.corr()\n",
    "top_corr_features=corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#g=sns.heatmap(df[top_corr_features].corr, annot='True',cmap='RdYlGn')\n",
    "g=sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "communist-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['horsepower','boreratio','enginesize','curbweight','curbweight','carwidth','carlength','wheelbase','drivewheel','enginelocation','price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "blond-height",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "horsepower        1.775003e-16\n",
       "boreratio        -5.910719e-15\n",
       "enginesize        2.897411e-17\n",
       "curbweight        1.261863e-16\n",
       "curbweight        1.261863e-16\n",
       "carwidth          1.430401e-14\n",
       "carlength        -1.011440e-14\n",
       "wheelbase        -2.013999e-14\n",
       "drivewheel       -1.646380e-16\n",
       "enginelocation   -1.371532e-16\n",
       "price             1.353931e-16\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=(df-df.mean())/df.std()\n",
    "df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cleared-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "colored-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalized_df=(df-df.mean())/df.std()\n",
    "normalized_df.mean(axis=0)\n",
    "y=normalized_df.price\n",
    "x=normalized_df.drop('price',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "genetic-virgin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 10)\n",
      "(102, 10)\n",
      "(72, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horsepower</th>\n",
       "      <th>boreratio</th>\n",
       "      <th>enginesize</th>\n",
       "      <th>curbweight</th>\n",
       "      <th>curbweight</th>\n",
       "      <th>carwidth</th>\n",
       "      <th>carlength</th>\n",
       "      <th>wheelbase</th>\n",
       "      <th>drivewheel</th>\n",
       "      <th>enginelocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2.601722</td>\n",
       "      <td>1.514689</td>\n",
       "      <td>1.611151</td>\n",
       "      <td>0.384947</td>\n",
       "      <td>0.384947</td>\n",
       "      <td>-0.423179</td>\n",
       "      <td>-0.417374</td>\n",
       "      <td>-1.537185</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>8.185651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.249921</td>\n",
       "      <td>1.662375</td>\n",
       "      <td>0.338419</td>\n",
       "      <td>0.728728</td>\n",
       "      <td>0.728728</td>\n",
       "      <td>0.602365</td>\n",
       "      <td>1.195622</td>\n",
       "      <td>0.920561</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>-0.559301</td>\n",
       "      <td>1.071629</td>\n",
       "      <td>-0.454037</td>\n",
       "      <td>-0.788518</td>\n",
       "      <td>-0.788518</td>\n",
       "      <td>-0.236716</td>\n",
       "      <td>-0.166104</td>\n",
       "      <td>-0.258493</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.275209</td>\n",
       "      <td>-0.516003</td>\n",
       "      <td>0.218350</td>\n",
       "      <td>0.515545</td>\n",
       "      <td>0.515545</td>\n",
       "      <td>0.229440</td>\n",
       "      <td>0.206750</td>\n",
       "      <td>0.106848</td>\n",
       "      <td>-2.385652</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-1.115640</td>\n",
       "      <td>-1.549809</td>\n",
       "      <td>-1.150438</td>\n",
       "      <td>-1.380052</td>\n",
       "      <td>-1.380052</td>\n",
       "      <td>-0.889335</td>\n",
       "      <td>-1.949316</td>\n",
       "      <td>-0.839717</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>-0.179978</td>\n",
       "      <td>0.480882</td>\n",
       "      <td>-0.165871</td>\n",
       "      <td>1.295294</td>\n",
       "      <td>1.295294</td>\n",
       "      <td>1.161752</td>\n",
       "      <td>2.014278</td>\n",
       "      <td>2.564595</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>-0.913335</td>\n",
       "      <td>-1.180593</td>\n",
       "      <td>-0.718189</td>\n",
       "      <td>-0.454340</td>\n",
       "      <td>-0.454340</td>\n",
       "      <td>-0.190101</td>\n",
       "      <td>-0.190420</td>\n",
       "      <td>-0.241886</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-0.179978</td>\n",
       "      <td>0.480882</td>\n",
       "      <td>-0.165871</td>\n",
       "      <td>0.997607</td>\n",
       "      <td>0.997607</td>\n",
       "      <td>1.161752</td>\n",
       "      <td>1.025406</td>\n",
       "      <td>1.518392</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>-0.559301</td>\n",
       "      <td>1.071629</td>\n",
       "      <td>-0.454037</td>\n",
       "      <td>-0.260363</td>\n",
       "      <td>-0.260363</td>\n",
       "      <td>-0.236716</td>\n",
       "      <td>-0.036415</td>\n",
       "      <td>-0.308312</td>\n",
       "      <td>-2.385652</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.078825</td>\n",
       "      <td>0.628569</td>\n",
       "      <td>-0.454037</td>\n",
       "      <td>-0.308377</td>\n",
       "      <td>-0.308377</td>\n",
       "      <td>-0.516410</td>\n",
       "      <td>0.222961</td>\n",
       "      <td>0.405763</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.508724</td>\n",
       "      <td>0.222431</td>\n",
       "      <td>-0.117843</td>\n",
       "      <td>-0.279569</td>\n",
       "      <td>-0.279569</td>\n",
       "      <td>0.276055</td>\n",
       "      <td>0.304016</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>-0.913335</td>\n",
       "      <td>-1.328279</td>\n",
       "      <td>-0.886286</td>\n",
       "      <td>-1.224486</td>\n",
       "      <td>-1.224486</td>\n",
       "      <td>-0.982566</td>\n",
       "      <td>-1.357613</td>\n",
       "      <td>-0.839717</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>-1.065064</td>\n",
       "      <td>-1.032906</td>\n",
       "      <td>-0.838258</td>\n",
       "      <td>-1.095809</td>\n",
       "      <td>-1.095809</td>\n",
       "      <td>-1.075797</td>\n",
       "      <td>-1.244136</td>\n",
       "      <td>-0.507589</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.300498</td>\n",
       "      <td>1.071629</td>\n",
       "      <td>0.458488</td>\n",
       "      <td>0.237063</td>\n",
       "      <td>0.237063</td>\n",
       "      <td>-0.143485</td>\n",
       "      <td>0.174328</td>\n",
       "      <td>-0.059216</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.174057</td>\n",
       "      <td>0.517804</td>\n",
       "      <td>0.074267</td>\n",
       "      <td>-0.014531</td>\n",
       "      <td>-0.014531</td>\n",
       "      <td>-0.842719</td>\n",
       "      <td>-0.425480</td>\n",
       "      <td>-1.686643</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>-1.317946</td>\n",
       "      <td>-1.180593</td>\n",
       "      <td>-0.718189</td>\n",
       "      <td>-0.559971</td>\n",
       "      <td>-0.559971</td>\n",
       "      <td>-0.190101</td>\n",
       "      <td>-0.190420</td>\n",
       "      <td>-0.241886</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>-0.483436</td>\n",
       "      <td>-0.516003</td>\n",
       "      <td>-0.430023</td>\n",
       "      <td>-0.665602</td>\n",
       "      <td>-0.665602</td>\n",
       "      <td>-0.190101</td>\n",
       "      <td>-0.190420</td>\n",
       "      <td>-0.241886</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.477515</td>\n",
       "      <td>0.923942</td>\n",
       "      <td>1.346999</td>\n",
       "      <td>2.332399</td>\n",
       "      <td>2.332399</td>\n",
       "      <td>2.700067</td>\n",
       "      <td>2.314182</td>\n",
       "      <td>2.797084</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.047616</td>\n",
       "      <td>-1.180593</td>\n",
       "      <td>0.434474</td>\n",
       "      <td>1.270327</td>\n",
       "      <td>1.270327</td>\n",
       "      <td>1.394830</td>\n",
       "      <td>1.195622</td>\n",
       "      <td>1.717669</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.508724</td>\n",
       "      <td>0.222431</td>\n",
       "      <td>-0.117843</td>\n",
       "      <td>-0.250760</td>\n",
       "      <td>-0.250760</td>\n",
       "      <td>0.276055</td>\n",
       "      <td>0.304016</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.862759</td>\n",
       "      <td>-1.106749</td>\n",
       "      <td>-0.886286</td>\n",
       "      <td>-1.308991</td>\n",
       "      <td>-1.308991</td>\n",
       "      <td>-1.075797</td>\n",
       "      <td>-1.471091</td>\n",
       "      <td>-0.706865</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.199345</td>\n",
       "      <td>-0.331394</td>\n",
       "      <td>-0.694175</td>\n",
       "      <td>-0.558051</td>\n",
       "      <td>-0.558051</td>\n",
       "      <td>-0.889335</td>\n",
       "      <td>-0.433585</td>\n",
       "      <td>-0.706865</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.407571</td>\n",
       "      <td>0.037822</td>\n",
       "      <td>-0.117843</td>\n",
       "      <td>-0.039498</td>\n",
       "      <td>-0.039498</td>\n",
       "      <td>-0.609641</td>\n",
       "      <td>0.044640</td>\n",
       "      <td>0.754497</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.458148</td>\n",
       "      <td>-0.663689</td>\n",
       "      <td>-0.406009</td>\n",
       "      <td>-0.352550</td>\n",
       "      <td>-0.352550</td>\n",
       "      <td>-1.588569</td>\n",
       "      <td>0.109484</td>\n",
       "      <td>-0.374738</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-0.230554</td>\n",
       "      <td>1.367002</td>\n",
       "      <td>0.602571</td>\n",
       "      <td>1.231916</td>\n",
       "      <td>1.231916</td>\n",
       "      <td>1.161752</td>\n",
       "      <td>1.025406</td>\n",
       "      <td>1.518392</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-0.407571</td>\n",
       "      <td>0.074744</td>\n",
       "      <td>-0.117843</td>\n",
       "      <td>-0.039498</td>\n",
       "      <td>-0.039498</td>\n",
       "      <td>-0.609641</td>\n",
       "      <td>0.044640</td>\n",
       "      <td>0.754497</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.053537</td>\n",
       "      <td>-1.106749</td>\n",
       "      <td>-0.694175</td>\n",
       "      <td>-0.821168</td>\n",
       "      <td>-0.821168</td>\n",
       "      <td>-0.982566</td>\n",
       "      <td>-1.357613</td>\n",
       "      <td>-0.839717</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-1.065064</td>\n",
       "      <td>-1.032906</td>\n",
       "      <td>-0.838258</td>\n",
       "      <td>1.064827</td>\n",
       "      <td>1.064827</td>\n",
       "      <td>-1.075797</td>\n",
       "      <td>-0.352530</td>\n",
       "      <td>-0.507589</td>\n",
       "      <td>-2.385652</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>-0.356995</td>\n",
       "      <td>-0.516003</td>\n",
       "      <td>-0.430023</td>\n",
       "      <td>-0.579177</td>\n",
       "      <td>-0.579177</td>\n",
       "      <td>-0.796104</td>\n",
       "      <td>-1.195503</td>\n",
       "      <td>-0.706865</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-0.407571</td>\n",
       "      <td>0.074744</td>\n",
       "      <td>-0.117843</td>\n",
       "      <td>-0.437055</td>\n",
       "      <td>-0.437055</td>\n",
       "      <td>-0.236716</td>\n",
       "      <td>-0.085049</td>\n",
       "      <td>-0.407950</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.817788</td>\n",
       "      <td>1.108550</td>\n",
       "      <td>3.148036</td>\n",
       "      <td>2.900886</td>\n",
       "      <td>2.900886</td>\n",
       "      <td>1.721140</td>\n",
       "      <td>2.071017</td>\n",
       "      <td>2.365318</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.300498</td>\n",
       "      <td>-0.589846</td>\n",
       "      <td>-0.406009</td>\n",
       "      <td>-0.293013</td>\n",
       "      <td>-0.293013</td>\n",
       "      <td>-0.236716</td>\n",
       "      <td>-0.133682</td>\n",
       "      <td>-0.407950</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.913335</td>\n",
       "      <td>-1.328279</td>\n",
       "      <td>-0.886286</td>\n",
       "      <td>-1.305150</td>\n",
       "      <td>-1.305150</td>\n",
       "      <td>-0.982566</td>\n",
       "      <td>-1.357613</td>\n",
       "      <td>-0.839717</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.078825</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>-1.366562</td>\n",
       "      <td>-0.327583</td>\n",
       "      <td>-0.327583</td>\n",
       "      <td>-0.096870</td>\n",
       "      <td>-0.409269</td>\n",
       "      <td>-0.574014</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>-0.862759</td>\n",
       "      <td>-0.516003</td>\n",
       "      <td>-0.694175</td>\n",
       "      <td>-0.886467</td>\n",
       "      <td>-0.886467</td>\n",
       "      <td>-0.702873</td>\n",
       "      <td>-0.628118</td>\n",
       "      <td>-0.507589</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.249921</td>\n",
       "      <td>1.662375</td>\n",
       "      <td>0.338419</td>\n",
       "      <td>0.934228</td>\n",
       "      <td>0.934228</td>\n",
       "      <td>0.602365</td>\n",
       "      <td>1.195622</td>\n",
       "      <td>0.920561</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>4.650065</td>\n",
       "      <td>2.253122</td>\n",
       "      <td>1.827276</td>\n",
       "      <td>1.556491</td>\n",
       "      <td>1.556491</td>\n",
       "      <td>2.979761</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>-0.059216</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.458148</td>\n",
       "      <td>-0.663689</td>\n",
       "      <td>-0.406009</td>\n",
       "      <td>-0.613747</td>\n",
       "      <td>-0.613747</td>\n",
       "      <td>-0.329948</td>\n",
       "      <td>-0.530852</td>\n",
       "      <td>-0.374738</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>-0.862759</td>\n",
       "      <td>-0.516003</td>\n",
       "      <td>-0.694175</td>\n",
       "      <td>-0.857659</td>\n",
       "      <td>-0.857659</td>\n",
       "      <td>-0.702873</td>\n",
       "      <td>-0.628118</td>\n",
       "      <td>-0.507589</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>-0.407571</td>\n",
       "      <td>-0.516003</td>\n",
       "      <td>-0.430023</td>\n",
       "      <td>0.014278</td>\n",
       "      <td>0.014278</td>\n",
       "      <td>0.462518</td>\n",
       "      <td>0.733608</td>\n",
       "      <td>0.272912</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.174057</td>\n",
       "      <td>0.517804</td>\n",
       "      <td>0.074267</td>\n",
       "      <td>-0.014531</td>\n",
       "      <td>-0.014531</td>\n",
       "      <td>-0.842719</td>\n",
       "      <td>-0.425480</td>\n",
       "      <td>-1.686643</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.913335</td>\n",
       "      <td>-1.106749</td>\n",
       "      <td>-0.862272</td>\n",
       "      <td>-1.249454</td>\n",
       "      <td>-1.249454</td>\n",
       "      <td>-0.796104</td>\n",
       "      <td>-1.211714</td>\n",
       "      <td>-0.939355</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.174057</td>\n",
       "      <td>1.071629</td>\n",
       "      <td>-0.454037</td>\n",
       "      <td>0.181367</td>\n",
       "      <td>0.181367</td>\n",
       "      <td>-0.236716</td>\n",
       "      <td>-0.036415</td>\n",
       "      <td>-0.308312</td>\n",
       "      <td>-2.385652</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>-0.356995</td>\n",
       "      <td>0.480882</td>\n",
       "      <td>0.122295</td>\n",
       "      <td>0.045007</td>\n",
       "      <td>0.045007</td>\n",
       "      <td>0.276055</td>\n",
       "      <td>0.603920</td>\n",
       "      <td>-0.441163</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>-1.065064</td>\n",
       "      <td>-1.032906</td>\n",
       "      <td>-0.838258</td>\n",
       "      <td>-0.990178</td>\n",
       "      <td>-0.990178</td>\n",
       "      <td>-1.075797</td>\n",
       "      <td>-1.244136</td>\n",
       "      <td>-0.507589</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.210872</td>\n",
       "      <td>0.370117</td>\n",
       "      <td>1.298972</td>\n",
       "      <td>0.968798</td>\n",
       "      <td>0.968798</td>\n",
       "      <td>0.276055</td>\n",
       "      <td>0.855190</td>\n",
       "      <td>0.272912</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.179978</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>-0.165871</td>\n",
       "      <td>-0.444737</td>\n",
       "      <td>-0.444737</td>\n",
       "      <td>-0.329948</td>\n",
       "      <td>-0.052626</td>\n",
       "      <td>-0.258493</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>-0.230554</td>\n",
       "      <td>1.367002</td>\n",
       "      <td>0.602571</td>\n",
       "      <td>1.337547</td>\n",
       "      <td>1.337547</td>\n",
       "      <td>1.161752</td>\n",
       "      <td>1.025406</td>\n",
       "      <td>1.518392</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.300498</td>\n",
       "      <td>-0.589846</td>\n",
       "      <td>-0.406009</td>\n",
       "      <td>-0.293013</td>\n",
       "      <td>-0.293013</td>\n",
       "      <td>-0.236716</td>\n",
       "      <td>-0.133682</td>\n",
       "      <td>-0.407950</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2.020094</td>\n",
       "      <td>1.736219</td>\n",
       "      <td>4.252671</td>\n",
       "      <td>2.226768</td>\n",
       "      <td>2.226768</td>\n",
       "      <td>2.839914</td>\n",
       "      <td>2.038595</td>\n",
       "      <td>2.199254</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-0.913335</td>\n",
       "      <td>-1.328279</td>\n",
       "      <td>-0.886286</td>\n",
       "      <td>-1.088126</td>\n",
       "      <td>-1.088126</td>\n",
       "      <td>-0.982566</td>\n",
       "      <td>-0.547063</td>\n",
       "      <td>-0.839717</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.249921</td>\n",
       "      <td>1.662375</td>\n",
       "      <td>0.338419</td>\n",
       "      <td>0.972640</td>\n",
       "      <td>0.972640</td>\n",
       "      <td>1.394830</td>\n",
       "      <td>1.195622</td>\n",
       "      <td>1.717669</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.862759</td>\n",
       "      <td>-1.106749</td>\n",
       "      <td>-0.886286</td>\n",
       "      <td>-1.241772</td>\n",
       "      <td>-1.241772</td>\n",
       "      <td>-1.075797</td>\n",
       "      <td>-1.236031</td>\n",
       "      <td>-0.706865</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>-0.104113</td>\n",
       "      <td>-0.516003</td>\n",
       "      <td>-0.430023</td>\n",
       "      <td>-0.490831</td>\n",
       "      <td>-0.490831</td>\n",
       "      <td>-0.190101</td>\n",
       "      <td>-0.190420</td>\n",
       "      <td>-0.241886</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.286737</td>\n",
       "      <td>0.480882</td>\n",
       "      <td>2.571704</td>\n",
       "      <td>2.274782</td>\n",
       "      <td>2.274782</td>\n",
       "      <td>2.700067</td>\n",
       "      <td>2.314182</td>\n",
       "      <td>2.797084</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.413178</td>\n",
       "      <td>0.370117</td>\n",
       "      <td>1.298972</td>\n",
       "      <td>1.120523</td>\n",
       "      <td>1.120523</td>\n",
       "      <td>0.928674</td>\n",
       "      <td>0.360754</td>\n",
       "      <td>0.073635</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.426938</td>\n",
       "      <td>-0.072943</td>\n",
       "      <td>0.890737</td>\n",
       "      <td>0.959196</td>\n",
       "      <td>0.959196</td>\n",
       "      <td>0.462518</td>\n",
       "      <td>1.211833</td>\n",
       "      <td>0.787710</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-0.888047</td>\n",
       "      <td>-0.663689</td>\n",
       "      <td>-0.718189</td>\n",
       "      <td>-1.186075</td>\n",
       "      <td>-1.186075</td>\n",
       "      <td>-0.982566</td>\n",
       "      <td>-0.709173</td>\n",
       "      <td>-0.706865</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.438466</td>\n",
       "      <td>-0.220629</td>\n",
       "      <td>1.058834</td>\n",
       "      <td>0.884294</td>\n",
       "      <td>0.884294</td>\n",
       "      <td>0.835443</td>\n",
       "      <td>0.766030</td>\n",
       "      <td>0.688072</td>\n",
       "      <td>1.210367</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.210872</td>\n",
       "      <td>0.370117</td>\n",
       "      <td>1.298972</td>\n",
       "      <td>1.422052</td>\n",
       "      <td>1.422052</td>\n",
       "      <td>0.276055</td>\n",
       "      <td>0.855190</td>\n",
       "      <td>0.272912</td>\n",
       "      <td>-0.587642</td>\n",
       "      <td>-0.121569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     horsepower  boreratio  enginesize  curbweight  curbweight  carwidth  \\\n",
       "126    2.601722   1.514689    1.611151    0.384947    0.384947 -0.423179   \n",
       "196    0.249921   1.662375    0.338419    0.728728    0.728728  0.602365   \n",
       "141   -0.559301   1.071629   -0.454037   -0.788518   -0.788518 -0.236716   \n",
       "4      0.275209  -0.516003    0.218350    0.515545    0.515545  0.229440   \n",
       "32    -1.115640  -1.549809   -1.150438   -1.380052   -1.380052 -0.889335   \n",
       "109   -0.179978   0.480882   -0.165871    1.295294    1.295294  1.161752   \n",
       "187   -0.913335  -1.180593   -0.718189   -0.454340   -0.454340 -0.190101   \n",
       "115   -0.179978   0.480882   -0.165871    0.997607    0.997607  1.161752   \n",
       "148   -0.559301   1.071629   -0.454037   -0.260363   -0.260363 -0.236716   \n",
       "10    -0.078825   0.628569   -0.454037   -0.308377   -0.308377 -0.516410   \n",
       "62    -0.508724   0.222431   -0.117843   -0.279569   -0.279569  0.276055   \n",
       "118   -0.913335  -1.328279   -0.886286   -1.224486   -1.224486 -0.982566   \n",
       "150   -1.065064  -1.032906   -0.838258   -1.095809   -1.095809 -1.075797   \n",
       "170    0.300498   1.071629    0.458488    0.237063    0.237063 -0.143485   \n",
       "0      0.174057   0.517804    0.074267   -0.014531   -0.014531 -0.842719   \n",
       "184   -1.317946  -1.180593   -0.718189   -0.559971   -0.559971 -0.190101   \n",
       "183   -0.483436  -0.516003   -0.430023   -0.665602   -0.665602 -0.190101   \n",
       "70     0.477515   0.923942    1.346999    2.332399    2.332399  2.700067   \n",
       "203    0.047616  -1.180593    0.434474    1.270327    1.270327  1.394830   \n",
       "64    -0.508724   0.222431   -0.117843   -0.250760   -0.250760  0.276055   \n",
       "44    -0.862759  -1.106749   -0.886286   -1.308991   -1.308991 -1.075797   \n",
       "165    0.199345  -0.331394   -0.694175   -0.558051   -0.558051 -0.889335   \n",
       "28    -0.407571   0.037822   -0.117843   -0.039498   -0.039498 -0.609641   \n",
       "40    -0.458148  -0.663689   -0.406009   -0.352550   -0.352550 -1.588569   \n",
       "108   -0.230554   1.367002    0.602571    1.231916    1.231916  1.161752   \n",
       "123   -0.407571   0.074744   -0.117843   -0.039498   -0.039498 -0.609641   \n",
       "23    -0.053537  -1.106749   -0.694175   -0.821168   -0.821168 -0.982566   \n",
       "155   -1.065064  -1.032906   -0.838258    1.064827    1.064827 -1.075797   \n",
       "189   -0.356995  -0.516003   -0.430023   -0.579177   -0.579177 -0.796104   \n",
       "81    -0.407571   0.074744   -0.117843   -0.437055   -0.437055 -0.236716   \n",
       "..          ...        ...         ...         ...         ...       ...   \n",
       "48     1.817788   1.108550    3.148036    2.900886    2.900886  1.721140   \n",
       "88     0.300498  -0.589846   -0.406009   -0.293013   -0.293013 -0.236716   \n",
       "21    -0.913335  -1.328279   -0.886286   -1.305150   -1.305150 -0.982566   \n",
       "57    -0.078825   0.000901   -1.366562   -0.327583   -0.327583 -0.096870   \n",
       "160   -0.862759  -0.516003   -0.694175   -0.886467   -0.886467 -0.702873   \n",
       "197    0.249921   1.662375    0.338419    0.934228    0.934228  0.602365   \n",
       "129    4.650065   2.253122    1.827276    1.556491    1.556491  2.979761   \n",
       "37    -0.458148  -0.663689   -0.406009   -0.613747   -0.613747 -0.329948   \n",
       "157   -0.862759  -0.516003   -0.694175   -0.857659   -0.857659 -0.702873   \n",
       "193   -0.407571  -0.516003   -0.430023    0.014278    0.014278  0.462518   \n",
       "1      0.174057   0.517804    0.074267   -0.014531   -0.014531 -0.842719   \n",
       "52    -0.913335  -1.106749   -0.862272   -1.249454   -1.249454 -0.796104   \n",
       "149    0.174057   1.071629   -0.454037    0.181367    0.181367 -0.236716   \n",
       "130   -0.356995   0.480882    0.122295    0.045007    0.045007  0.276055   \n",
       "151   -1.065064  -1.032906   -0.838258   -0.990178   -0.990178 -1.075797   \n",
       "103    1.210872   0.370117    1.298972    0.968798    0.968798  0.276055   \n",
       "99    -0.179978   0.000901   -0.165871   -0.444737   -0.444737 -0.329948   \n",
       "116   -0.230554   1.367002    0.602571    1.337547    1.337547  1.161752   \n",
       "87     0.300498  -0.589846   -0.406009   -0.293013   -0.293013 -0.236716   \n",
       "74     2.020094   1.736219    4.252671    2.226768    2.226768  2.839914   \n",
       "121   -0.913335  -1.328279   -0.886286   -1.088126   -1.088126 -0.982566   \n",
       "204    0.249921   1.662375    0.338419    0.972640    0.972640  1.394830   \n",
       "20    -0.862759  -1.106749   -0.886286   -1.241772   -1.241772 -1.075797   \n",
       "188   -0.104113  -0.516003   -0.430023   -0.490831   -0.490831 -0.190101   \n",
       "71     1.286737   0.480882    2.571704    2.274782    2.274782  2.700067   \n",
       "106    1.413178   0.370117    1.298972    1.120523    1.120523  0.928674   \n",
       "14     0.426938  -0.072943    0.890737    0.959196    0.959196  0.462518   \n",
       "92    -0.888047  -0.663689   -0.718189   -1.186075   -1.186075 -0.982566   \n",
       "179    1.438466  -0.220629    1.058834    0.884294    0.884294  0.835443   \n",
       "102    1.210872   0.370117    1.298972    1.422052    1.422052  0.276055   \n",
       "\n",
       "     carlength  wheelbase  drivewheel  enginelocation  \n",
       "126  -0.417374  -1.537185    1.210367        8.185651  \n",
       "196   1.195622   0.920561    1.210367       -0.121569  \n",
       "141  -0.166104  -0.258493   -0.587642       -0.121569  \n",
       "4     0.206750   0.106848   -2.385652       -0.121569  \n",
       "32   -1.949316  -0.839717   -0.587642       -0.121569  \n",
       "109   2.014278   2.564595    1.210367       -0.121569  \n",
       "187  -0.190420  -0.241886   -0.587642       -0.121569  \n",
       "115   1.025406   1.518392    1.210367       -0.121569  \n",
       "148  -0.036415  -0.308312   -2.385652       -0.121569  \n",
       "10    0.222961   0.405763    1.210367       -0.121569  \n",
       "62    0.304016   0.007210   -0.587642       -0.121569  \n",
       "118  -1.357613  -0.839717   -0.587642       -0.121569  \n",
       "150  -1.244136  -0.507589   -0.587642       -0.121569  \n",
       "170   0.174328  -0.059216    1.210367       -0.121569  \n",
       "0    -0.425480  -1.686643    1.210367       -0.121569  \n",
       "184  -0.190420  -0.241886   -0.587642       -0.121569  \n",
       "183  -0.190420  -0.241886   -0.587642       -0.121569  \n",
       "70    2.314182   2.797084    1.210367       -0.121569  \n",
       "203   1.195622   1.717669    1.210367       -0.121569  \n",
       "64    0.304016   0.007210   -0.587642       -0.121569  \n",
       "44   -1.471091  -0.706865   -0.587642       -0.121569  \n",
       "165  -0.433585  -0.706865    1.210367       -0.121569  \n",
       "28    0.044640   0.754497   -0.587642       -0.121569  \n",
       "40    0.109484  -0.374738   -0.587642       -0.121569  \n",
       "108   1.025406   1.518392    1.210367       -0.121569  \n",
       "123   0.044640   0.754497   -0.587642       -0.121569  \n",
       "23   -1.357613  -0.839717   -0.587642       -0.121569  \n",
       "155  -0.352530  -0.507589   -2.385652       -0.121569  \n",
       "189  -1.195503  -0.706865   -0.587642       -0.121569  \n",
       "81   -0.085049  -0.407950   -0.587642       -0.121569  \n",
       "..         ...        ...         ...             ...  \n",
       "48    2.071017   2.365318    1.210367       -0.121569  \n",
       "88   -0.133682  -0.407950   -0.587642       -0.121569  \n",
       "21   -1.357613  -0.839717   -0.587642       -0.121569  \n",
       "57   -0.409269  -0.574014    1.210367       -0.121569  \n",
       "160  -0.628118  -0.507589   -0.587642       -0.121569  \n",
       "197   1.195622   0.920561    1.210367       -0.121569  \n",
       "129   0.133800  -0.059216    1.210367       -0.121569  \n",
       "37   -0.530852  -0.374738   -0.587642       -0.121569  \n",
       "157  -0.628118  -0.507589   -0.587642       -0.121569  \n",
       "193   0.733608   0.272912   -0.587642       -0.121569  \n",
       "1    -0.425480  -1.686643    1.210367       -0.121569  \n",
       "52   -1.211714  -0.939355   -0.587642       -0.121569  \n",
       "149  -0.036415  -0.308312   -2.385652       -0.121569  \n",
       "130   0.603920  -0.441163   -0.587642       -0.121569  \n",
       "151  -1.244136  -0.507589   -0.587642       -0.121569  \n",
       "103   0.855190   0.272912   -0.587642       -0.121569  \n",
       "99   -0.052626  -0.258493   -0.587642       -0.121569  \n",
       "116   1.025406   1.518392    1.210367       -0.121569  \n",
       "87   -0.133682  -0.407950   -0.587642       -0.121569  \n",
       "74    2.038595   2.199254    1.210367       -0.121569  \n",
       "121  -0.547063  -0.839717   -0.587642       -0.121569  \n",
       "204   1.195622   1.717669    1.210367       -0.121569  \n",
       "20   -1.236031  -0.706865   -0.587642       -0.121569  \n",
       "188  -0.190420  -0.241886   -0.587642       -0.121569  \n",
       "71    2.314182   2.797084    1.210367       -0.121569  \n",
       "106   0.360754   0.073635    1.210367       -0.121569  \n",
       "14    1.211833   0.787710    1.210367       -0.121569  \n",
       "92   -0.709173  -0.706865   -0.587642       -0.121569  \n",
       "179   0.766030   0.688072    1.210367       -0.121569  \n",
       "102   0.855190   0.272912   -0.587642       -0.121569  \n",
       "\n",
       "[102 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.5,random_state=42)\n",
    "x_train.head()\n",
    "x_test,x_validate,y_test,y_validate=train_test_split(x_test,y_test,test_size=0.3)\n",
    "print(x_validate.shape)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accurate-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers,optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(20,input_dim=(x_train.shape[1]),kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(layers.Dense(8))\n",
    "    model.add(layers.Dense(6,activation='tanh'))\n",
    "    #optimizer = optimizers.RMSprop(0.001)\n",
    "    model.add(Dropout(0.3))\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae'])\n",
    "    return model\n",
    "#checked using both activation relu gives more error than tanh that is why using tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "american-packet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "21/21 [==============================] - 0s 7ms/step - mae: 0.8622 - loss: 1.4930 - val_mae: 0.6345 - val_loss: 1.1743\n",
      "Epoch 2/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.7684 - loss: 1.2499 - val_mae: 0.5516 - val_loss: 0.9433\n",
      "Epoch 3/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.6842 - loss: 1.0405 - val_mae: 0.4927 - val_loss: 0.8080\n",
      "Epoch 4/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.6729 - loss: 0.9844 - val_mae: 0.4593 - val_loss: 0.7431\n",
      "Epoch 5/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.6404 - loss: 0.8794 - val_mae: 0.4419 - val_loss: 0.7050\n",
      "Epoch 6/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.6122 - loss: 0.8106 - val_mae: 0.4224 - val_loss: 0.6759\n",
      "Epoch 7/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5934 - loss: 0.8212 - val_mae: 0.4139 - val_loss: 0.6587\n",
      "Epoch 8/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.6056 - loss: 0.8233 - val_mae: 0.4099 - val_loss: 0.6503\n",
      "Epoch 9/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.6040 - loss: 0.7910 - val_mae: 0.4053 - val_loss: 0.6462\n",
      "Epoch 10/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5948 - loss: 0.8023 - val_mae: 0.4007 - val_loss: 0.6366\n",
      "Epoch 11/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5784 - loss: 0.7360 - val_mae: 0.4018 - val_loss: 0.6329\n",
      "Epoch 12/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5641 - loss: 0.7142 - val_mae: 0.4015 - val_loss: 0.6273\n",
      "Epoch 13/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5662 - loss: 0.7589 - val_mae: 0.3983 - val_loss: 0.6230\n",
      "Epoch 14/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5614 - loss: 0.7448 - val_mae: 0.3960 - val_loss: 0.6163\n",
      "Epoch 15/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5375 - loss: 0.7031 - val_mae: 0.4003 - val_loss: 0.6197\n",
      "Epoch 16/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5252 - loss: 0.6802 - val_mae: 0.4037 - val_loss: 0.6180\n",
      "Epoch 17/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5355 - loss: 0.6866 - val_mae: 0.4095 - val_loss: 0.6228\n",
      "Epoch 18/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5024 - loss: 0.6312 - val_mae: 0.4143 - val_loss: 0.6234\n",
      "Epoch 19/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4964 - loss: 0.6063 - val_mae: 0.4040 - val_loss: 0.6079\n",
      "Epoch 20/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5287 - loss: 0.7017 - val_mae: 0.4117 - val_loss: 0.6101\n",
      "Epoch 21/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5259 - loss: 0.6533 - val_mae: 0.4103 - val_loss: 0.6070\n",
      "Epoch 22/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5201 - loss: 0.6840 - val_mae: 0.4151 - val_loss: 0.6078\n",
      "Epoch 23/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4949 - loss: 0.6121 - val_mae: 0.4116 - val_loss: 0.5987\n",
      "Epoch 24/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5175 - loss: 0.6751 - val_mae: 0.4108 - val_loss: 0.6024\n",
      "Epoch 25/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4802 - loss: 0.6179 - val_mae: 0.4176 - val_loss: 0.6090\n",
      "Epoch 26/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5052 - loss: 0.6280 - val_mae: 0.4101 - val_loss: 0.5964\n",
      "Epoch 27/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4812 - loss: 0.5515 - val_mae: 0.4233 - val_loss: 0.6110\n",
      "Epoch 28/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5078 - loss: 0.6427 - val_mae: 0.4173 - val_loss: 0.6015\n",
      "Epoch 29/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4886 - loss: 0.5841 - val_mae: 0.4162 - val_loss: 0.5990\n",
      "Epoch 30/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4710 - loss: 0.5764 - val_mae: 0.4116 - val_loss: 0.5874\n",
      "Epoch 31/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4806 - loss: 0.5525 - val_mae: 0.4153 - val_loss: 0.5950\n",
      "Epoch 32/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5007 - loss: 0.6424 - val_mae: 0.4178 - val_loss: 0.5949\n",
      "Epoch 33/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4858 - loss: 0.5883 - val_mae: 0.4143 - val_loss: 0.5918\n",
      "Epoch 34/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4917 - loss: 0.6070 - val_mae: 0.4242 - val_loss: 0.5981\n",
      "Epoch 35/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4796 - loss: 0.5756 - val_mae: 0.4176 - val_loss: 0.5916\n",
      "Epoch 36/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4759 - loss: 0.5938 - val_mae: 0.4108 - val_loss: 0.5834\n",
      "Epoch 37/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4697 - loss: 0.5407 - val_mae: 0.4299 - val_loss: 0.6038\n",
      "Epoch 38/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4753 - loss: 0.6028 - val_mae: 0.4210 - val_loss: 0.5887\n",
      "Epoch 39/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4835 - loss: 0.5368 - val_mae: 0.4264 - val_loss: 0.5953\n",
      "Epoch 40/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4907 - loss: 0.5931 - val_mae: 0.4096 - val_loss: 0.5758\n",
      "Epoch 41/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4727 - loss: 0.5343 - val_mae: 0.4339 - val_loss: 0.6036\n",
      "Epoch 42/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4910 - loss: 0.5982 - val_mae: 0.4211 - val_loss: 0.5833\n",
      "Epoch 43/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4892 - loss: 0.6178 - val_mae: 0.4247 - val_loss: 0.5839\n",
      "Epoch 44/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4965 - loss: 0.6210 - val_mae: 0.4247 - val_loss: 0.5859\n",
      "Epoch 45/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4801 - loss: 0.5904 - val_mae: 0.4197 - val_loss: 0.5807\n",
      "Epoch 46/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4878 - loss: 0.6058 - val_mae: 0.4307 - val_loss: 0.5918\n",
      "Epoch 47/150\n",
      "21/21 [==============================] - 0s 3ms/step - mae: 0.4935 - loss: 0.5927 - val_mae: 0.4193 - val_loss: 0.5802\n",
      "Epoch 48/150\n",
      "21/21 [==============================] - 0s 3ms/step - mae: 0.4745 - loss: 0.5504 - val_mae: 0.4105 - val_loss: 0.5680\n",
      "Epoch 49/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4824 - loss: 0.5793 - val_mae: 0.4221 - val_loss: 0.5840\n",
      "Epoch 50/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4699 - loss: 0.5466 - val_mae: 0.4126 - val_loss: 0.5727\n",
      "Epoch 51/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4841 - loss: 0.5844 - val_mae: 0.4377 - val_loss: 0.5955\n",
      "Epoch 52/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4899 - loss: 0.5711 - val_mae: 0.4373 - val_loss: 0.5954\n",
      "Epoch 53/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4994 - loss: 0.6048 - val_mae: 0.4151 - val_loss: 0.5699\n",
      "Epoch 54/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4899 - loss: 0.6067 - val_mae: 0.4438 - val_loss: 0.5995\n",
      "Epoch 55/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4857 - loss: 0.5792 - val_mae: 0.4379 - val_loss: 0.5926\n",
      "Epoch 56/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5007 - loss: 0.6183 - val_mae: 0.4247 - val_loss: 0.5794\n",
      "Epoch 57/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4775 - loss: 0.5420 - val_mae: 0.4231 - val_loss: 0.5797\n",
      "Epoch 58/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4765 - loss: 0.5951 - val_mae: 0.4206 - val_loss: 0.5739\n",
      "Epoch 59/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4793 - loss: 0.5449 - val_mae: 0.4241 - val_loss: 0.5750\n",
      "Epoch 60/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4783 - loss: 0.6029 - val_mae: 0.4137 - val_loss: 0.5637\n",
      "Epoch 61/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4881 - loss: 0.5913 - val_mae: 0.4177 - val_loss: 0.5697\n",
      "Epoch 62/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5072 - loss: 0.6399 - val_mae: 0.4162 - val_loss: 0.5683\n",
      "Epoch 63/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4739 - loss: 0.5948 - val_mae: 0.4268 - val_loss: 0.5749\n",
      "Epoch 64/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4937 - loss: 0.6269 - val_mae: 0.4147 - val_loss: 0.5605\n",
      "Epoch 65/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4540 - loss: 0.5251 - val_mae: 0.4201 - val_loss: 0.5685\n",
      "Epoch 66/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4737 - loss: 0.5734 - val_mae: 0.4086 - val_loss: 0.5543\n",
      "Epoch 67/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.5039 - loss: 0.5924 - val_mae: 0.4176 - val_loss: 0.5666\n",
      "Epoch 68/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4970 - loss: 0.6019 - val_mae: 0.4075 - val_loss: 0.5518\n",
      "Epoch 69/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4797 - loss: 0.5763 - val_mae: 0.4215 - val_loss: 0.5655\n",
      "Epoch 70/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4621 - loss: 0.5309 - val_mae: 0.4372 - val_loss: 0.5849\n",
      "Epoch 71/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4692 - loss: 0.5802 - val_mae: 0.4198 - val_loss: 0.5675\n",
      "Epoch 72/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4832 - loss: 0.5443 - val_mae: 0.4235 - val_loss: 0.5706\n",
      "Epoch 73/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4775 - loss: 0.5852 - val_mae: 0.4271 - val_loss: 0.5738\n",
      "Epoch 74/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4826 - loss: 0.5871 - val_mae: 0.4270 - val_loss: 0.5751\n",
      "Epoch 75/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4671 - loss: 0.5343 - val_mae: 0.4151 - val_loss: 0.5593\n",
      "Epoch 76/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4962 - loss: 0.5859 - val_mae: 0.4151 - val_loss: 0.5625\n",
      "Epoch 77/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4792 - loss: 0.5834 - val_mae: 0.4109 - val_loss: 0.5539\n",
      "Epoch 78/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4537 - loss: 0.5014 - val_mae: 0.4296 - val_loss: 0.5742\n",
      "Epoch 79/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4728 - loss: 0.5440 - val_mae: 0.4278 - val_loss: 0.5674\n",
      "Epoch 80/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4819 - loss: 0.5786 - val_mae: 0.4204 - val_loss: 0.5569\n",
      "Epoch 81/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4732 - loss: 0.5594 - val_mae: 0.4338 - val_loss: 0.5750\n",
      "Epoch 82/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4719 - loss: 0.5483 - val_mae: 0.4431 - val_loss: 0.5883\n",
      "Epoch 83/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4915 - loss: 0.5794 - val_mae: 0.4377 - val_loss: 0.5828\n",
      "Epoch 84/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4768 - loss: 0.5725 - val_mae: 0.4241 - val_loss: 0.5657\n",
      "Epoch 85/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4764 - loss: 0.5517 - val_mae: 0.4288 - val_loss: 0.5717\n",
      "Epoch 86/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4746 - loss: 0.6024 - val_mae: 0.4275 - val_loss: 0.5693\n",
      "Epoch 87/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4814 - loss: 0.5793 - val_mae: 0.4404 - val_loss: 0.5813\n",
      "Epoch 88/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4776 - loss: 0.5570 - val_mae: 0.4319 - val_loss: 0.5730\n",
      "Epoch 89/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4825 - loss: 0.5984 - val_mae: 0.4380 - val_loss: 0.5826\n",
      "Epoch 90/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4827 - loss: 0.5981 - val_mae: 0.4357 - val_loss: 0.5806\n",
      "Epoch 91/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4618 - loss: 0.5234 - val_mae: 0.4197 - val_loss: 0.5592\n",
      "Epoch 92/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4631 - loss: 0.5043 - val_mae: 0.4149 - val_loss: 0.5526\n",
      "Epoch 93/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4610 - loss: 0.5393 - val_mae: 0.4265 - val_loss: 0.5660\n",
      "Epoch 94/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4645 - loss: 0.5138 - val_mae: 0.4326 - val_loss: 0.5729\n",
      "Epoch 95/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4758 - loss: 0.5286 - val_mae: 0.4319 - val_loss: 0.5730\n",
      "Epoch 96/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4767 - loss: 0.5371 - val_mae: 0.4388 - val_loss: 0.5821\n",
      "Epoch 97/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4928 - loss: 0.5663 - val_mae: 0.4338 - val_loss: 0.5791\n",
      "Epoch 98/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4670 - loss: 0.5209 - val_mae: 0.4356 - val_loss: 0.5784\n",
      "Epoch 99/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4687 - loss: 0.5638 - val_mae: 0.4220 - val_loss: 0.5600\n",
      "Epoch 100/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4698 - loss: 0.5217 - val_mae: 0.4248 - val_loss: 0.5669\n",
      "Epoch 101/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4781 - loss: 0.5918 - val_mae: 0.4202 - val_loss: 0.5526\n",
      "Epoch 102/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4802 - loss: 0.5763 - val_mae: 0.4416 - val_loss: 0.5831\n",
      "Epoch 103/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4774 - loss: 0.5936 - val_mae: 0.4244 - val_loss: 0.5661\n",
      "Epoch 104/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4694 - loss: 0.5078 - val_mae: 0.4301 - val_loss: 0.5673\n",
      "Epoch 105/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4593 - loss: 0.5152 - val_mae: 0.4234 - val_loss: 0.5600\n",
      "Epoch 106/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4707 - loss: 0.5320 - val_mae: 0.4166 - val_loss: 0.5503\n",
      "Epoch 107/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4743 - loss: 0.5823 - val_mae: 0.4322 - val_loss: 0.5700\n",
      "Epoch 108/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4865 - loss: 0.6089 - val_mae: 0.4388 - val_loss: 0.5801\n",
      "Epoch 109/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4831 - loss: 0.6114 - val_mae: 0.4223 - val_loss: 0.5580\n",
      "Epoch 110/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4847 - loss: 0.5612 - val_mae: 0.4283 - val_loss: 0.5666\n",
      "Epoch 111/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4607 - loss: 0.5003 - val_mae: 0.4309 - val_loss: 0.5709\n",
      "Epoch 112/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4869 - loss: 0.5761 - val_mae: 0.4381 - val_loss: 0.5811\n",
      "Epoch 113/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4711 - loss: 0.5475 - val_mae: 0.4283 - val_loss: 0.5695\n",
      "Epoch 114/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4732 - loss: 0.5140 - val_mae: 0.4209 - val_loss: 0.5586\n",
      "Epoch 115/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4656 - loss: 0.5242 - val_mae: 0.4167 - val_loss: 0.5539\n",
      "Epoch 116/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4756 - loss: 0.5879 - val_mae: 0.4299 - val_loss: 0.5678\n",
      "Epoch 117/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4737 - loss: 0.5652 - val_mae: 0.4216 - val_loss: 0.5557\n",
      "Epoch 118/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4426 - loss: 0.5186 - val_mae: 0.4155 - val_loss: 0.5452\n",
      "Epoch 119/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4920 - loss: 0.5845 - val_mae: 0.4259 - val_loss: 0.5608\n",
      "Epoch 120/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4809 - loss: 0.5871 - val_mae: 0.4214 - val_loss: 0.5582\n",
      "Epoch 121/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4783 - loss: 0.5640 - val_mae: 0.4124 - val_loss: 0.5422\n",
      "Epoch 122/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4763 - loss: 0.5550 - val_mae: 0.4110 - val_loss: 0.5440\n",
      "Epoch 123/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4766 - loss: 0.5505 - val_mae: 0.4262 - val_loss: 0.5668\n",
      "Epoch 124/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4687 - loss: 0.5309 - val_mae: 0.4156 - val_loss: 0.5506\n",
      "Epoch 125/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4787 - loss: 0.5506 - val_mae: 0.4172 - val_loss: 0.5515\n",
      "Epoch 126/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4620 - loss: 0.5604 - val_mae: 0.4439 - val_loss: 0.5850\n",
      "Epoch 127/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4640 - loss: 0.5157 - val_mae: 0.4155 - val_loss: 0.5536\n",
      "Epoch 128/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4708 - loss: 0.5309 - val_mae: 0.4089 - val_loss: 0.5442\n",
      "Epoch 129/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4601 - loss: 0.5395 - val_mae: 0.4194 - val_loss: 0.5541\n",
      "Epoch 130/150\n",
      "21/21 [==============================] - 0s 3ms/step - mae: 0.4757 - loss: 0.5491 - val_mae: 0.4304 - val_loss: 0.5655\n",
      "Epoch 131/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4884 - loss: 0.5776 - val_mae: 0.4378 - val_loss: 0.5740\n",
      "Epoch 132/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4825 - loss: 0.5643 - val_mae: 0.4222 - val_loss: 0.5577\n",
      "Epoch 133/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4757 - loss: 0.5732 - val_mae: 0.4248 - val_loss: 0.5604\n",
      "Epoch 134/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4888 - loss: 0.5538 - val_mae: 0.4180 - val_loss: 0.5533\n",
      "Epoch 135/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4994 - loss: 0.6220 - val_mae: 0.4177 - val_loss: 0.5499\n",
      "Epoch 136/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4820 - loss: 0.5616 - val_mae: 0.4244 - val_loss: 0.5603\n",
      "Epoch 137/150\n",
      "21/21 [==============================] - ETA: 0s - mae: 0.8183 - loss: 1.862 - 0s 2ms/step - mae: 0.4789 - loss: 0.5430 - val_mae: 0.4367 - val_loss: 0.5736\n",
      "Epoch 138/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4671 - loss: 0.5680 - val_mae: 0.4417 - val_loss: 0.5755\n",
      "Epoch 139/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4924 - loss: 0.5767 - val_mae: 0.4326 - val_loss: 0.5701\n",
      "Epoch 140/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4939 - loss: 0.5496 - val_mae: 0.4162 - val_loss: 0.5476\n",
      "Epoch 141/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4918 - loss: 0.5828 - val_mae: 0.4146 - val_loss: 0.5443\n",
      "Epoch 142/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4916 - loss: 0.5300 - val_mae: 0.4195 - val_loss: 0.5520\n",
      "Epoch 143/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4707 - loss: 0.5371 - val_mae: 0.4345 - val_loss: 0.5694\n",
      "Epoch 144/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4618 - loss: 0.5186 - val_mae: 0.4262 - val_loss: 0.5610\n",
      "Epoch 145/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4787 - loss: 0.5586 - val_mae: 0.4190 - val_loss: 0.5556\n",
      "Epoch 146/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4554 - loss: 0.4941 - val_mae: 0.4455 - val_loss: 0.5840\n",
      "Epoch 147/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4982 - loss: 0.5959 - val_mae: 0.4417 - val_loss: 0.5796\n",
      "Epoch 148/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4897 - loss: 0.5967 - val_mae: 0.4246 - val_loss: 0.5606\n",
      "Epoch 149/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4805 - loss: 0.5590 - val_mae: 0.4177 - val_loss: 0.5514\n",
      "Epoch 150/150\n",
      "21/21 [==============================] - 0s 2ms/step - mae: 0.4702 - loss: 0.5370 - val_mae: 0.4278 - val_loss: 0.5605\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "history = model.fit(x_train,y_train,epochs=150,batch_size=5,validation_data=(x_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hundred-anime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "certain-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(x_train) // k\n",
    "num_epochs = 100\n",
    "all_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "controlled-monitoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 1\n",
      "WARNING:tensorflow:6 out of the last 1055 calls to <function Model.make_test_function.<locals>.test_function at 0x00000229AF84B730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score for fold =  1 mse=  0.5990230441093445 loss 0.4524846374988556\n",
      "processing fold # 2\n",
      "WARNING:tensorflow:7 out of the last 1056 calls to <function Model.make_test_function.<locals>.test_function at 0x00000229AC550E18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score for fold =  2 mse=  0.5828668475151062 loss 0.4270685017108917\n",
      "processing fold # 3\n",
      "WARNING:tensorflow:8 out of the last 1057 calls to <function Model.make_test_function.<locals>.test_function at 0x00000229B1AE2158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score for fold =  3 mse=  0.5807279944419861 loss 0.436581552028656\n",
      "processing fold # 4\n",
      "WARNING:tensorflow:9 out of the last 1058 calls to <function Model.make_test_function.<locals>.test_function at 0x00000229B2AFB400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score for fold =  4 mse=  0.5569414496421814 loss 0.4110918939113617\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i+1)\n",
    "    partial_train_data = np.concatenate([x_train[:i * num_val_samples],x_train[(i + 1) * num_val_samples:]],axis=0)\n",
    "    partial_train_targets = np.concatenate([y_train[:i * num_val_samples],y_train[(i + 1) * num_val_samples:]],axis=0)\n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=5, verbose=0)\n",
    "    scores= model.evaluate(x_validate, y_validate, verbose=0)\n",
    "    print(\"Score for fold = \",i+1,\"mse= \",scores[0],\"loss\",scores[1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "reflected-compact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "And the value is  0.1539873\n"
     ]
    }
   ],
   "source": [
    "score1= model.predict(x_train)\n",
    "print('\\nAnd the value is ', np.mean(score1[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
